# ğŸ§ª AI Prompt Testing Framework

This project demonstrates automated prompt testing for various OpenAI models (e.g., `gpt-3.5-turbo`, `gpt-4.1-nano`). It evaluates AI responses based on tone, accuracy, translation, and formatting.

## âœ… Features

* Load test cases from code
* Execute prompts with specified tone and model
* Store results in `test_results.csv`
* Basic Pass/Fail validation with comments

## ğŸ“ Files

* `test_prompt_api.py` â€” main test runner
* `test_results.csv` â€” output log of test cases
* `.env` â€” your OpenAI API key
* `requirements.txt` â€” dependencies

## ğŸ§ª Sample Test Cases

Below is a preview of test results:

| Prompt                                   | Tone   | Model        | Output                            | Pass | Comment |
| ---------------------------------------- | ------ | ------------ | --------------------------------- | ---- | ------- |
| Write a polite email to cancel a meeting | polite | gpt-4.1-nano | "Subject: Cancellation..."        | âœ…    | OK      |
| Translate into Polish: I can't attend    | polite | gpt-4.1-nano | Nie mogÄ™ uczestniczyÄ‡ w spotkaniu | âœ…    | OK      |
| Make this short: I'm sorry, I can't come | short  | gpt-4.1-nano | Sorry, I can't make it.           | âœ…    | OK      |

ğŸ‘‰ See all test results in [`test_results.csv`](test_results.csv)

## ğŸ§° Setup

```bash
pip install -r requirements.txt
```

Create a `.env` file:

```
OPENAI_API_KEY=your_api_key_here
```

Run tests:

```bash
python test_prompt_api.py
```

## ğŸ’¡ Use Case

This project is useful for:

* Testing prompt stability
* Verifying multilingual output
* Experimenting with tone-based prompt engineering
* AI QA portfolios

---
